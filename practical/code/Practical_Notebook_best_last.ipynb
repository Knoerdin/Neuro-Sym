{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19197c84",
   "metadata": {},
   "source": [
    "# Practical Worksheet\n",
    "\n",
    "In this worksheet, we will be working with a small dataset of hyponym-hypernym pairs. Hyponymy is the `is-a` relation. So we will have pairs like `(cat, mammal)` meaning 'A cat is a mammal'. The hyponym is the more specific term (e.g., cat) and the hypernym is the more general term (e.g., mammal). In this notebook you will:\n",
    "\n",
    "1. (3 pts) Use Logical Neural Networks with a very small hyponym dataset to infer a set of facts. You will discuss the kinds of facts that you can infer and the limitations of the model as it is implemented\n",
    "2. (5 pts) Set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
    "3. (5 pts) Evaluate the effect of different axioms in the LTN system.\n",
    "4. (2 pts) Query your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15d0af",
   "metadata": {},
   "source": [
    "## Part 0. Setup\n",
    "Create an environment and install python 3.12, numpy, pandas, and scikit-learn.\n",
    "\n",
    "Install LNNs using `pip install git+https://github.com/IBM/LNN`\n",
    "\n",
    "Install LTNs using `pip install LTNtorch`\n",
    "\n",
    "Import packages as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03699930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ltntorch in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ltntorch) (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ltntorch) (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->ltntorch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->ltntorch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->ltntorch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->ltntorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->ltntorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch->ltntorch) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch->ltntorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch->ltntorch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\thijn\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "!pip install ltntorch\n",
    "import ltn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84644f44",
   "metadata": {},
   "source": [
    "## Part 1. Inferring facts using Logical Neural Networks\n",
    "\n",
    "In this first part, we will manually specify a very small dictionary of hyponym facts. We have three hyponyms and three non-hyponyms. The hyponymy relation is transitive, meaning that if $x$ is a hyponym of $y$ and $y$ is a hyponym of $z$, then $x$ should be a hyponym of $z$.\n",
    "\n",
    "You will:\n",
    "\n",
    "a. (1.5 pt) Set up a LNN model with suitable variables, a transitivity axiom, and hyponymy data.\n",
    "\n",
    "b. (0.5 pt) Run inference over the model.\n",
    "\n",
    "c. (1 pt) Inspect the output of the model and discuss whether the output is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032bfc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/IBM/LNN 'C:\\Users\\thijn\\AppData\\Local\\Temp\\pip-req-build-iuapkhup'\n",
      "  WARNING: The script wsdump.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script send2trash.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pyjson5.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pybabel.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-execute.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-events.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-console.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jupyter-dejavu.exe and jupyter-nbconvert.exe are installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-server.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts jlpm.exe, jupyter-lab.exe, jupyter-labextension.exe and jupyter-labhub.exe are installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jupyter-notebook.exe is installed in 'C:\\Users\\thijn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.5.1+cu118 requires torch==2.5.1+cu118, but you have torch 2.9.1 which is incompatible.\n",
      "torchvision 0.20.1 requires torch==2.5.1, but you have torch 2.9.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\thijn\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/IBM/LNN\n",
      "  Cloning https://github.com/IBM/LNN to c:\\users\\thijn\\appdata\\local\\temp\\pip-req-build-iuapkhup\n",
      "  Resolved https://github.com/IBM/LNN to commit 18ea03a52a79e6bbe8dada76e1ad9b320cd894d4\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jupyter (from lnn==1.0)\n",
      "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.3 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (3.9.2)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.23.4 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.3.4 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (2.2.3)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (78.1.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (0.9.0)\n",
      "Collecting torch>=2.7.1 (from lnn==1.0)\n",
      "  Downloading torch-2.9.1-cp311-cp311-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from lnn==1.0) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib>=3.3.3->lnn==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.3.4->lnn==1.0) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=1.3.4->lnn==1.0) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.7.1->lnn==1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.7.1->lnn==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.7.1->lnn==1.0) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.7.1->lnn==1.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.7.1->lnn==1.0) (2024.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.66.3->lnn==1.0) (0.4.6)\n",
      "Collecting notebook (from jupyter->lnn==1.0)\n",
      "  Downloading notebook-7.5.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jupyter-console (from jupyter->lnn==1.0)\n",
      "  Using cached jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nbconvert (from jupyter->lnn==1.0)\n",
      "  Using cached nbconvert-7.16.6-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter->lnn==1.0) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter->lnn==1.0) (8.1.5)\n",
      "Collecting jupyterlab (from jupyter->lnn==1.0)\n",
      "  Downloading jupyterlab-4.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.3->lnn==1.0) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch>=2.7.1->lnn==1.0) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (1.8.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipykernel->jupyter->lnn==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->jupyter->lnn==1.0) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipywidgets->jupyter->lnn==1.0) (3.0.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=2.7.1->lnn==1.0) (3.0.2)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-console->jupyter->lnn==1.0) (3.0.48)\n",
      "Requirement already satisfied: pygments in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-console->jupyter->lnn==1.0) (2.18.0)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting httpx<1,>=0.25.0 (from jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached jupyter_lsp-2.3.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached jupyter_server-2.17.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.28.0 (from jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached jupyterlab_server-2.28.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting notebook-shim>=0.2 (from jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting beautifulsoup4 (from nbconvert->jupyter->lnn==1.0)\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting bleach!=5.0.0 (from bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached bleach-6.3.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting defusedxml (from nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting jupyterlab-pygments (from nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached mistune-3.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting nbclient>=0.5.0 (from nbconvert->jupyter->lnn==1.0)\n",
      "  Downloading nbclient-0.10.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nbconvert->jupyter->lnn==1.0) (5.10.4)\n",
      "Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting webencodings (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0 (from bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting anyio (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.19.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.6.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->lnn==1.0) (4.3.6)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->lnn==1.0) (308)\n",
      "Collecting argon2-cffi>=21.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached argon2_cffi-25.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting prometheus-client>=0.9 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached prometheus_client-0.23.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pywinpty>=2.0.1 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Downloading pywinpty-3.0.2-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting send2trash>=1.8.2 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached Send2Trash-1.8.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting terminado>=0.8.3 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached terminado-0.18.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting websocket-client>=1.7 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting babel>=2.10 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nbformat>=5.7->nbconvert->jupyter->lnn==1.0) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->lnn==1.0) (0.2.13)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4->nbconvert->jupyter->lnn==1.0)\n",
      "  Downloading soupsieve-2.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting argon2-cffi-bindings (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.22.3)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached python_json_logger-4.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (6.0.2)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.3.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.2.3)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jsonpointer>1.13 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting webcolors>=24.6.0 (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached webcolors-25.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\thijn\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (2.22)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0)\n",
      "  Using cached arrow-1.4.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Downloading torch-2.9.1-cp311-cp311-win_amd64.whl (111.0 MB)\n",
      "   ---------------------------------------- 0.0/111.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 8.9/111.0 MB 46.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 26.0/111.0 MB 65.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 41.2/111.0 MB 67.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 50.1/111.0 MB 62.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 57.4/111.0 MB 55.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 64.2/111.0 MB 51.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 71.8/111.0 MB 49.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 78.4/111.0 MB 46.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 83.6/111.0 MB 44.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 89.7/111.0 MB 42.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 95.7/111.0 MB 41.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 102.0/111.0 MB 40.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  108.5/111.0 MB 39.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 39.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/111.0 MB 39.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 111.0/111.0 MB 32.8 MB/s eta 0:00:00\n",
      "Using cached jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Using cached jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading jupyterlab-4.5.1-py3-none-any.whl (12.4 MB)\n",
      "   ---------------------------------------- 0.0/12.4 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 7.3/12.4 MB 32.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.4/12.4 MB 28.7 MB/s eta 0:00:00\n",
      "Using cached nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Downloading notebook-7.5.1-py3-none-any.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 7.6/14.5 MB 36.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.4/14.5 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 31.4 MB/s eta 0:00:00\n",
      "Using cached async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Using cached bleach-6.3.0-py3-none-any.whl (164 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jupyter_lsp-2.3.0-py3-none-any.whl (76 kB)\n",
      "Using cached jupyter_server-2.17.0-py3-none-any.whl (388 kB)\n",
      "Using cached jupyterlab_server-2.28.0-py3-none-any.whl (59 kB)\n",
      "Using cached mistune-3.1.4-py3-none-any.whl (53 kB)\n",
      "Downloading nbclient-0.10.3-py3-none-any.whl (25 kB)\n",
      "Using cached notebook_shim-0.2.4-py3-none-any.whl (13 kB)\n",
      "Using cached pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "Using cached argon2_cffi-25.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "Using cached json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Using cached jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached prometheus_client-0.23.1-py3-none-any.whl (61 kB)\n",
      "Downloading pywinpty-3.0.2-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 22.9 MB/s eta 0:00:00\n",
      "Using cached Send2Trash-1.8.3-py3-none-any.whl (18 kB)\n",
      "Downloading soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
      "Using cached terminado-0.18.1-py3-none-any.whl (14 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached python_json_logger-4.0.0-py3-none-any.whl (15 kB)\n",
      "Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached argon2_cffi_bindings-25.1.0-cp39-abi3-win_amd64.whl (31 kB)\n",
      "Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached webcolors-25.10.0-py3-none-any.whl (14 kB)\n",
      "Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Using cached uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Using cached arrow-1.4.0-py3-none-any.whl (68 kB)\n",
      "Building wheels for collected packages: lnn\n",
      "  Building wheel for lnn (setup.py): started\n",
      "  Building wheel for lnn (setup.py): finished with status 'done'\n",
      "  Created wheel for lnn: filename=lnn-1.0-py3-none-any.whl size=80832 sha256=edffe6550aa675a440fee034cf3cad0cb7c08a8707327a0efb6dc64ecea0582e\n",
      "  Stored in directory: C:\\Users\\thijn\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-0s8z4d4k\\wheels\\45\\a4\\c1\\ed27677834d9fd0b740698e027829b34975fdb2d1ebdfa3e2c\n",
      "Successfully built lnn\n",
      "Installing collected packages: webencodings, websocket-client, webcolors, uri-template, tinycss2, soupsieve, send2trash, rfc3986-validator, rfc3339-validator, pywinpty, python-json-logger, prometheus-client, pandocfilters, overrides, mistune, jupyterlab-pygments, jsonpointer, json5, h11, fqdn, defusedxml, bleach, babel, async-lru, anyio, torch, terminado, httpcore, beautifulsoup4, arrow, argon2-cffi-bindings, jupyter-server-terminals, isoduration, httpx, argon2-cffi, nbclient, jupyter-events, jupyter-console, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, jupyter-lsp, jupyterlab, notebook, jupyter, lnn\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.7.0\n",
      "    Uninstalling torch-2.7.0:\n",
      "      Successfully uninstalled torch-2.7.0\n",
      "Successfully installed anyio-4.12.0 argon2-cffi-25.1.0 argon2-cffi-bindings-25.1.0 arrow-1.4.0 async-lru-2.0.5 babel-2.17.0 beautifulsoup4-4.14.3 bleach-6.3.0 defusedxml-0.7.1 fqdn-1.5.1 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 isoduration-20.11.0 json5-0.12.1 jsonpointer-3.0.0 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-events-0.12.0 jupyter-lsp-2.3.0 jupyter-server-2.17.0 jupyter-server-terminals-0.5.3 jupyterlab-4.5.1 jupyterlab-pygments-0.3.0 jupyterlab-server-2.28.0 lnn-1.0 mistune-3.1.4 nbclient-0.10.3 nbconvert-7.16.6 notebook-7.5.1 notebook-shim-0.2.4 overrides-7.7.0 pandocfilters-1.5.1 prometheus-client-0.23.1 python-json-logger-4.0.0 pywinpty-3.0.2 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 send2trash-1.8.3 soupsieve-2.8.1 terminado-0.18.1 tinycss2-1.4.0 torch-2.9.1 uri-template-1.3.0 webcolors-25.10.0 webencodings-0.5.1 websocket-client-1.9.0\n"
     ]
    }
   ],
   "source": [
    "# We first set up a small dictionary of hyponyms\n",
    "!pip install git+https://github.com/IBM/LNN\n",
    "from lnn import Fact\n",
    "\n",
    "hyp_dict = {('cat', 'mammal'):Fact.TRUE,\n",
    "            ('dog', 'mammal'):Fact.TRUE,\n",
    "            ('mammal', 'animal'):Fact.TRUE,\n",
    "            ('cat', 'dog'):Fact.FALSE,\n",
    "            ('animal', 'mammal'):Fact.FALSE,\n",
    "            ('mammal', 'dog'):Fact.FALSE,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafd55d",
   "metadata": {},
   "source": [
    "### Part 1a) (1.5 pts) Setting up the model.\n",
    "Set up a LNN model with suitable predicates and variables, a transitivity axiom, and hyponymy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a13684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty model\n",
    "from lnn import Model\n",
    "model = Model()\n",
    "from lnn import Propositions, And, Implies, Iff, Fact, Model, Or\n",
    "A, B, C, D, E = Propositions(\"A\", \"B\", \"C\", \"D\", \"E\")\n",
    "IMPLIES=Implies(A, B)\n",
    "AND=And(C, D)\n",
    "IFF=Iff(AND, E)\n",
    "SENTENCE =And(IMPLIES, IFF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1697655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predicate of arity 2 called Hyps and three variables x, y, z\n",
    "## YOUR CODE HERE ##\n",
    "from lnn import Predicate, Variable\n",
    "Hyps = Predicate('Hyps', arity=2)\n",
    "x = Variable('x')\n",
    "y = Variable('y')\n",
    "z = Variable('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d25b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logical rule that encodes the fact that the hyponymy relation is transitive\n",
    "## YOUR CODE HERE ##\n",
    "transitivity_rule = Implies(And(Hyps(x, y), Hyps(y, z)), Hyps(x, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc34a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model()\n"
     ]
    }
   ],
   "source": [
    "# Add the knowledge and the data (the hyponymy dict) to the model and print.\n",
    "## YOUR CODE HERE ##\n",
    "model.add_knowledge(transitivity_rule)\n",
    "model.add_data({Hyps: {\n",
    "    ('cat', 'mammal'): Fact.TRUE,\n",
    "    ('dog', 'mammal'): Fact.TRUE,\n",
    "    ('mammal', 'animal'): Fact.TRUE,\n",
    "    ('cat', 'dog'): Fact.FALSE,\n",
    "    ('animal', 'mammal'): Fact.FALSE,\n",
    "    ('mammal', 'dog'): Fact.FALSE,\n",
    "}})\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42cce3",
   "metadata": {},
   "source": [
    "### Part 1b) (0.5 pts) Inferring facts\n",
    "Run inference over the model and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bf7202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************************************************************\n",
      "                                LNN Model\n",
      "\n",
      "OPEN Implies: ((Hyps(0, 1) ∧ Hyps(1, 2)) → Hyps(0, 2)) \n",
      "('dog', 'mammal', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('cat', 'cat', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('dog', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'cat')                                       TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'animal')                              TRUE (1.0, 1.0)\n",
      "('dog', 'animal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('cat', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('cat', 'dog', 'animal')                                    TRUE (1.0, 1.0)\n",
      "('cat', 'animal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('mammal', 'mammal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'mammal')                           UNKNOWN (0.0, 1.0)\n",
      "('cat', 'mammal', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('animal', 'cat', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('animal', 'mammal', 'cat')                                 TRUE (1.0, 1.0)\n",
      "('cat', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('animal', 'animal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'mammal')                              TRUE (1.0, 1.0)\n",
      "('mammal', 'mammal', 'animal')                              TRUE (1.0, 1.0)\n",
      "('mammal', 'cat', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('dog', 'dog', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'dog', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'animal')                              TRUE (1.0, 1.0)\n",
      "('dog', 'dog', 'dog')                                    UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'dog')                                       TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'dog')                                       TRUE (1.0, 1.0)\n",
      "('animal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'cat')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'mammal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('cat', 'mammal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'dog')                                       TRUE (1.0, 1.0)\n",
      "('cat', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'dog', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('dog', 'dog', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'mammal', 'dog')                                 TRUE (1.0, 1.0)\n",
      "('dog', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'mammal')                              TRUE (1.0, 1.0)\n",
      "('dog', 'cat', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'mammal')                              TRUE (1.0, 1.0)\n",
      "('animal', 'mammal', 'dog')                                 TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN And: (Hyps(0, 1) ∧ Hyps(1, 2)) \n",
      "('dog', 'mammal', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('cat', 'cat', 'mammal')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'animal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'mammal')                                FALSE (0.0, 0.0)\n",
      "('cat', 'dog', 'mammal')                                   FALSE (0.0, 0.0)\n",
      "('cat', 'dog', 'animal')                                   FALSE (0.0, 0.0)\n",
      "('cat', 'animal', 'mammal')                                FALSE (0.0, 0.0)\n",
      "('cat', 'mammal', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('animal', 'cat', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('mammal', 'dog', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('animal', 'mammal', 'mammal')                             FALSE (0.0, 0.0)\n",
      "('mammal', 'mammal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('dog', 'mammal', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'animal')                             FALSE (0.0, 0.0)\n",
      "('dog', 'mammal', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('dog', 'cat', 'dog')                                      FALSE (0.0, 0.0)\n",
      "('mammal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'dog')                                      FALSE (0.0, 0.0)\n",
      "('cat', 'mammal', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'mammal')                                FALSE (0.0, 0.0)\n",
      "('cat', 'mammal', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('cat', 'dog', 'dog')                                      FALSE (0.0, 0.0)\n",
      "('mammal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'animal')                                FALSE (0.0, 0.0)\n",
      "('animal', 'dog', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'mammal')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'dog')                                FALSE (0.0, 0.0)\n",
      "('animal', 'animal', 'mammal')                             FALSE (0.0, 0.0)\n",
      "('dog', 'cat', 'mammal')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'animal', 'mammal')                             FALSE (0.0, 0.0)\n",
      "('animal', 'mammal', 'dog')                                FALSE (0.0, 0.0)\n",
      "('animal', 'mammal', 'cat')                                FALSE (0.0, 0.0)\n",
      "('cat', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'mammal')                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'cat')                                      FALSE (0.0, 0.0)\n",
      "('animal', 'dog', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'dog')                                    UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'cat')                                   FALSE (0.0, 0.0)\n",
      "\n",
      "OPEN Predicate: Hyps \n",
      "('cat', 'dog')                                             FALSE (0.0, 0.0)\n",
      "('dog', 'mammal')                                           TRUE (1.0, 1.0)\n",
      "('animal', 'mammal')                                       FALSE (0.0, 0.0)\n",
      "('mammal', 'animal')                                        TRUE (1.0, 1.0)\n",
      "('cat', 'mammal')                                           TRUE (1.0, 1.0)\n",
      "('mammal', 'dog')                                          FALSE (0.0, 0.0)\n",
      "('cat', 'cat')                                           UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal')                                        UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat')                                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog')                                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal')                                        UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat')                                        UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog')                                        UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal')                                     UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat')                                        UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal')                                     UNKNOWN (0.0, 1.0)\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Part 1b (0.5 pts) Run inference over the model and print the output \n",
    "## YOUR CODE HERE ##\n",
    "model.infer()\n",
    "model.print()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0f233",
   "metadata": {},
   "source": [
    "### Part 1c) (1 pt) Inspecting the output.\n",
    "\n",
    "You should see that there are various facts whose truth value is unknown. \n",
    "\n",
    "\n",
    "Q1: Why can we not infer the truth value of all facts with the given database and axioms?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2: Suggest a suitable axiom to add to this system that would help to infer more facts. You do not need to implement the axiom.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84851554",
   "metadata": {},
   "source": [
    "\n",
    "Q1: Sometimes we do not knwo anything about all facts involved such as (mammel, mammel) and (mammel, cat) then we can not know (mammel, cat) since we have none of the fax and these are never mentioned so we cannot learn them either\n",
    "\n",
    "\n",
    "Q2: Implies(Hyps(x,y), not(Hyps(y,x))) if one is in one group sach cat in mammel then mammel not in cat. This should help create more false examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b131b6",
   "metadata": {},
   "source": [
    "## Part 2 (5 pts) Building Embeddings with Logic Tensor Networks.\n",
    "In this part, we will build a Logic Tensor Network to learn embeddings for the hyponyms. You will:\n",
    "\n",
    "a. (1 pt) Describe why learning embeddings for the hyponyms is a suitable approach.\n",
    "\n",
    "b. (1 pt) Set up a predicate for the hyponymy relation.\n",
    "\n",
    "c. (1 pt) Train a simple network on the hyponymy task.\n",
    "\n",
    "d. (2 pts) Assess satisfaction on the test set  and negative sample set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd42761",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "\n",
    "Below, we import the data into pandas dataframes. Take a look at the data to familiarise yourself with the format. In each .csv file we have a list of word pairs. \n",
    "- In train_hypernyms we have the set of hypernym pairs we will train on. \n",
    "- In test_hypernyms we have the set of pairs we will test on. \n",
    "- In non_hypernyms we have a set of word pairs that are not hypernym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36661dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs:\n",
      "[['supermarket' 'commercial building']\n",
      " ['hand tool' 'tool']\n",
      " ['peach' 'fruit']\n",
      " ['pike' 'fish']\n",
      " ['nail gun' 'power tool']]\n",
      "Testing pairs:\n",
      "[['workshop' 'building']\n",
      " ['train' 'vehicle']\n",
      " ['pine' 'physical object']\n",
      " ['snare drum' 'physical object']\n",
      " ['grape' 'physical object']]\n",
      "Negative pairs:\n",
      "[['jigsaw' 'nail gun']\n",
      " ['temple' 'synagogue']\n",
      " ['double bass' 'banjo']\n",
      " ['turkey' 'turkey']\n",
      " ['crocodile' 'snake']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../data/train_hypernyms.csv')\n",
    "test_df = pd.read_csv('../data/test_hypernyms.csv')\n",
    "neg_df = pd.read_csv('../data/non_hypernyms.csv')\n",
    "\n",
    "\n",
    "train_pairs = train_df.values\n",
    "test_pairs = test_df.values\n",
    "neg_pairs = neg_df.values\n",
    "\n",
    "print(\"Training pairs:\")\n",
    "print(train_pairs[:5])\n",
    "\n",
    "print(\"Testing pairs:\")\n",
    "print(test_pairs[:5])\n",
    "\n",
    "print(\"Negative pairs:\")\n",
    "print(neg_pairs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ad7aa",
   "metadata": {},
   "source": [
    "### Part 2a. (1 pt) Learning Embeddings\n",
    "\n",
    "When we use a logic tensor network, we can choose to use data from outside sources or to train embeddings within the network. We will be training embeddings. Do you think this is a suitable approach for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d647aef",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "This is a suitable aproach for the data set since we do have embeddings in the data so we should either train or get an universal embedder but those already contain relation information. Giving a lot of unwanted information to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e33e8",
   "metadata": {},
   "source": [
    "Below, we will set up the vocabulary and the initial random word embeddings to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da40ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a set of vocab by taking the union of the hyponyms and hypernyms\n",
    "vocab = set(train_df.hyper.unique()).union(train_df.hypo.unique())\n",
    "\n",
    "# Set the dimension of the vocab to 10\n",
    "vocab_dim = 10\n",
    "\n",
    "# Build a dictionary of word embeddings initialised randomly and set to be trainable.\n",
    "word_embeddings = {word: ltn.Constant(torch.rand((vocab_dim,)), trainable=True) \\\n",
    "                   for word in vocab}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f706ae5",
   "metadata": {},
   "source": [
    "### Part 2b. (1 pt) Defining a predicate.\n",
    "Define a predicate as a feed-forward NN with ELU and sigmoid activation functions and one hidden layer of size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c8835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feed-forward NN  with ELU and sigmoid activation functions and one hidden layer of size 16.\n",
    "class ModelHyp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        ## YOUR CODE HERE ##    \n",
    "        super(ModelHyp, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.fc1 = torch.nn.Linear(2 * vocab_dim, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # Specify the forward pass with ELU on the hidden layers and sigmoid on the output\n",
    "        x = list(x)\n",
    "        x = torch.cat(x, dim=1)\n",
    "        ## YOUR CODE HERE ##\n",
    "        x = self.fc1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# Wrap the feed-forward NN to make it an LTN predicate called Hyp\n",
    "Hyp = ltn.Predicate(ModelHyp())\n",
    "x = ltn.Variable('x', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "y = ltn.Variable('y', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "\n",
    "# Define connectives, quantifiers, and SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e276e36",
   "metadata": {},
   "source": [
    "### Part 2c. (1 pt) Training the network\n",
    "\n",
    "We set up a simple network in which we view our knowledge base as consisting just of those pairs in the training set. So our knowledge base states that for each word pair in the training set, this is a hyponym pair. We want to maximise the satisfaction over this knowledge base. To do this, we write a suitable axiom to aggregate the satisfaction of the hyponymy predicate over these pairs, and train the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e5c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\ltn\\core.py:363: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Set up a training loop for 300 epochs\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m300\u001b[39m):    \n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\u001b[39;00m\n\u001b[32m      8\u001b[39m     sat_agg = SatAgg(\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\u001b[39;00m\n\u001b[32m     10\u001b[39m         \u001b[38;5;66;03m## YOUR CODE HERE ##\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         *[\u001b[43mHyp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_pairs]\n\u001b[32m     12\u001b[39m         ,\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m         \u001b[38;5;66;03m# Our list of hyponym pairs is in train_pairs.\u001b[39;00m\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# We want to maximise the satisfaction gained by inputting the embeddings of those words into\u001b[39;00m\n\u001b[32m     17\u001b[39m         \u001b[38;5;66;03m# our hyponymy predicate\u001b[39;00m\n\u001b[32m     18\u001b[39m \n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m     )\n\u001b[32m     23\u001b[39m     loss = \u001b[32m1.\u001b[39m - sat_agg\n\u001b[32m     24\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\ltn\\core.py:616\u001b[39m, in \u001b[36mPredicate.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    613\u001b[39m proc_objs, output_vars, output_shape = process_ltn_objects(inputs)\n\u001b[32m    615\u001b[39m \u001b[38;5;66;03m# the management of the input is left to the model or the lambda function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproc_objs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# check if output of predicate contains only truth values, namely values in the range [0., 1.]\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.all(torch.where(torch.logical_and(output >= \u001b[32m0.\u001b[39m, output <= \u001b[32m1.\u001b[39m), \u001b[32m1.\u001b[39m, \u001b[32m0.\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mModelHyp.forward\u001b[39m\u001b[34m(self, *x)\u001b[39m\n\u001b[32m     17\u001b[39m x = torch.cat(x, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m## YOUR CODE HERE ##\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m x = \u001b[38;5;28mself\u001b[39m.elu(x)\n\u001b[32m     21\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# We have to optimize the parameters of the predicate and also of the embeddings\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "for epoch in range(300):    \n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    sat_agg = SatAgg(\n",
    "# Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs]\n",
    "        ,\n",
    "        \n",
    "\n",
    "        # Our list of hyponym pairs is in train_pairs.\n",
    "        # We want to maximise the satisfaction gained by inputting the embeddings of those words into\n",
    "        # our hyponymy predicate\n",
    "        \n",
    "        \n",
    "\n",
    "    )\n",
    "    \n",
    "    loss = 1. - sat_agg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41705558",
   "metadata": {},
   "source": [
    "### Part 2d (2 pts) Assessing the satisfaction on the test set\n",
    "\n",
    "Calculate the satisfaction over the test set using SatAgg. Do you think the model is generalising well? Now calculate the satisfaction over the negative samples dataset. Is this a suitable satisfaction level? Why or why not?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "The model performs well on the test set, having a satisfaction of 0.99 which means that it has an almost perfect generelisability to the test set. This is the same for the negative emmbeddings as those also have a high score of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90053da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocab = set(test_df.hyper.unique()).union(test_df.hypo.unique())\n",
    "test_x = ltn.Variable('x', torch.stack([word_embeddings[word].value for word in test_vocab]))\n",
    "test_y = ltn.Variable('y', torch.stack([word_embeddings[word].value for word in test_vocab]))\n",
    "neg_vocab = set(neg_df.hyper.unique()).union(neg_df.hypo.unique())\n",
    "neg_x = ltn.Variable('x', torch.stack([word_embeddings[word].value for word in neg_vocab]))\n",
    "neg_y = ltn.Variable('y', torch.stack([word_embeddings[word].value for word in neg_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c63e4d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m satisfaction_test = SatAgg(*[\u001b[43mHyp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m test_pairs],)\n\u001b[32m      2\u001b[39m satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m neg_pairs],)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe satisfaction of the test dataset is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msatisfaction_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\ltn\\core.py:616\u001b[39m, in \u001b[36mPredicate.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    613\u001b[39m proc_objs, output_vars, output_shape = process_ltn_objects(inputs)\n\u001b[32m    615\u001b[39m \u001b[38;5;66;03m# the management of the input is left to the model or the lambda function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproc_objs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# check if output of predicate contains only truth values, namely values in the range [0., 1.]\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.all(torch.where(torch.logical_and(output >= \u001b[32m0.\u001b[39m, output <= \u001b[32m1.\u001b[39m), \u001b[32m1.\u001b[39m, \u001b[32m0.\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mModelHyp.forward\u001b[39m\u001b[34m(self, *x)\u001b[39m\n\u001b[32m     17\u001b[39m x = torch.cat(x, dim=\u001b[32m1\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m## YOUR CODE HERE ##\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m x = \u001b[38;5;28mself\u001b[39m.elu(x)\n\u001b[32m     21\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\thijn\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but got mat1 is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "satisfaction_test = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in test_pairs],)\n",
    "satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs],)\n",
    "print(f\"the satisfaction of the test dataset is: {satisfaction_test}\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: {satisfaction_neg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b0390",
   "metadata": {},
   "source": [
    "## Part 3. (5 pts) Evaluate the effect of different axioms in the LTN system\n",
    "\n",
    "In this part you will:\n",
    "\n",
    "a. (2 pts) Retrain the model and evaluate the performance with negation included\n",
    "\n",
    "b. (2 pts) Retrain the model and evaluate performance with transitivity included\n",
    "\n",
    "c. (1 pt) Discuss the effect of the different axioms introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aaad1a",
   "metadata": {},
   "source": [
    "### Part 3a. (2pts)  Retraining the model with negation\n",
    "Reinitialise the model and retrain, including information from the `neg_pairs` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac747fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialise the model\n",
    "Hyp = ltn.Predicate(ModelHyp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473942d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.5194828510284424 | Train Sat 0.4805171489715576\n",
      " epoch 20 | loss 0.3263695240020752 | Train Sat 0.6736304759979248\n",
      " epoch 40 | loss 0.14462226629257202 | Train Sat 0.855377733707428\n",
      " epoch 60 | loss 0.04430818557739258 | Train Sat 0.9556918144226074\n",
      " epoch 80 | loss 0.009901165962219238 | Train Sat 0.9900988340377808\n",
      " epoch 100 | loss 0.001838088035583496 | Train Sat 0.9981619119644165\n",
      " epoch 120 | loss 0.0003591179847717285 | Train Sat 0.9996408820152283\n",
      " epoch 140 | loss 0.0001322031021118164 | Train Sat 0.9998677968978882\n",
      " epoch 160 | loss 0.00010347366333007812 | Train Sat 0.9998965263366699\n",
      " epoch 180 | loss 0.00010031461715698242 | Train Sat 0.999899685382843\n",
      " epoch 200 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 220 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 240 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 260 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 280 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "for epoch in range(300):\n",
    "    \n",
    "    \n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "        ## YOUR CODE HERE ##\n",
    "        sat_agg = SatAgg(\n",
    "        # Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "        \n",
    "\n",
    "        # Implement one axiom which aggregates the satisfaction across the (x, y) in neg_pairs\n",
    "        # Note that this statement should involve a negation.\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs],)\n",
    "\n",
    "    # Calculate the loss and propagate backwards\n",
    "    ## YOUR CODE HERE ##\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    ## YOUR CODE HERE ##\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768815fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the satisfaction of the test dataset is: 0.9998999834060669\n",
      "the satisfaction of the negative dataset is: 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "# Calculate the satisfaction across the test dataset and the negated dataset\n",
    "satisfaction_test = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in test_pairs])\n",
    "satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs])\n",
    "print(f\"the satisfaction of the test dataset is: {satisfaction_test.item()}\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: {satisfaction_neg.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbc5e2",
   "metadata": {},
   "source": [
    "### Part 3b. (2 pts) Retraining the model with transitivity\n",
    "\n",
    "As we discussed in Part 1, the hyponymy relation is transitive. This should be reflected in the axioms. Reinitialise the model and add an axiom expressing the rule:\n",
    "\n",
    "$\\forall x, y, z Hyp(x, y) \\land Hyp(y, z) \\implies Hyp(x, z)$\n",
    "\n",
    "Retrain the model and evaluate on the test and negated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialise the model\n",
    "## YOUR CODE HERE ##\n",
    "Hyp = ltn.Predicate(ModelHyp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.5601631999015808 | Train Sat 0.4398368000984192\n",
      " epoch 20 | loss 0.48356544971466064 | Train Sat 0.5164345502853394\n",
      " epoch 40 | loss 0.4884018898010254 | Train Sat 0.5115981101989746\n",
      " epoch 60 | loss 0.5044569969177246 | Train Sat 0.4955430030822754\n",
      " epoch 80 | loss 0.46948033571243286 | Train Sat 0.5305196642875671\n",
      " epoch 100 | loss 0.4353257417678833 | Train Sat 0.5646742582321167\n",
      " epoch 120 | loss 0.4845607876777649 | Train Sat 0.5154392123222351\n",
      " epoch 140 | loss 0.44327878952026367 | Train Sat 0.5567212104797363\n",
      " epoch 160 | loss 0.36909210681915283 | Train Sat 0.6309078931808472\n",
      " epoch 180 | loss 0.4273664355278015 | Train Sat 0.5726335644721985\n",
      " epoch 200 | loss 0.38229233026504517 | Train Sat 0.6177076697349548\n",
      " epoch 220 | loss 0.2256374955177307 | Train Sat 0.7743625044822693\n",
      " epoch 240 | loss 0.4212794899940491 | Train Sat 0.5787205100059509\n",
      " epoch 260 | loss 0.34680402278900146 | Train Sat 0.6531959772109985\n",
      " epoch 280 | loss 0.13517379760742188 | Train Sat 0.8648262023925781\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "## YOUR CODE HERE ##\n",
    "for epoch in range(300):    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "    x_ = ltn.Variable('x_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    y_ = ltn.Variable('y_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    z_ = ltn.Variable('z_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "    sat_agg = SatAgg(\n",
    "        \n",
    "        #Positive instances of hyponymy\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "\n",
    "        #Negative instances of hyponymy\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Not(Hyp(word_embeddings[x], word_embeddings[y])) for x, y in neg_pairs],\n",
    "        \n",
    "        # Transitivity axiom\n",
    "        ## YOUR CODE HERE ##\n",
    "        Forall([x_, y_, z_],\n",
    "                Implies(And(Hyp(x_, y_), Hyp(y_, z_)), Hyp(x_, z_))\n",
    "\n",
    "    ))\n",
    "    # Calculate the loss and propagate backwards\n",
    "    ## YOUR CODE HERE ##\n",
    "    loss = 1. - sat_agg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    ## YOUR CODE HERE ##\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcefe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the satisfaction of the test dataset is: 0.9955769181251526\n",
      "the satisfaction of the negative dataset is: 0.3272130489349365\n"
     ]
    }
   ],
   "source": [
    "# Calculate the satisfaction across the test dataset and the negated dataset\n",
    "satisfaction_test = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in test_pairs])\n",
    "satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs])\n",
    "print(f\"the satisfaction of the test dataset is: {satisfaction_test.item()}\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: {satisfaction_neg.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d1248",
   "metadata": {},
   "source": [
    "### Part 3c. (1 pt)  Evaluating the model\n",
    "How has the satisfaction changed across the test set and the set of negative examples as you include different axioms? Why has this happened? Write a couple of sentences with your conclusions about the datasets and the model you have built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed9e73",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "The satisfaction is great on the test set meaning the model does understand transitivity over new data, however the model struggles with the negative examples on the transitivity data set, this migth happen since we do not have a lot of pure non transitive examples. meaning that the model does not see wrong examples often enough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded3b40",
   "metadata": {},
   "source": [
    "## Part 4 (2 pts) Querying the model\n",
    "\n",
    "One of the strengths of Logic Tensor Networks is that you are able to query the models you have built. In this part you will:\n",
    "\n",
    "a. (0.5 pts) Define a logical statement that you expect to hold in your model.\n",
    "\n",
    "b. (1 pt) Query the model.\n",
    "\n",
    "c. (0.5 pts) Discuss your result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a26fbd",
   "metadata": {},
   "source": [
    "### Part 4a. (0.5 pts) Defining a query\n",
    "\n",
    "Thinking about the properties of hyponymy, give a logical statement that you would expect to hold in your model. The statement can be quite simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a874b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "Since no word is a hyponym of themselfs we can say that Hyp(x,y) -> Not(Hyp(y,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53450e",
   "metadata": {},
   "source": [
    "### Part 4b. (1 pt) Querying the model\n",
    "\n",
    "Write a function that returns the satisfaction level of your logical statement and determine the satisfaction level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the satisfaction level of your logical formula\n",
    "def phi():\n",
    "    # Create variables p, q, and r and initialize with the values from 'word_embeddings'\n",
    "    ## YOUR CODE HERE ##\n",
    "    p = ltn.Variable('p', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    q = ltn.Variable('q', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    # Return the truth value of phi\n",
    "    ## YOUR CODE HERE ##\n",
    "    return Forall([p, q], \n",
    "                  Implies(Hyp(p, q), Not(Hyp(q, p))))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The evaluation of phi is: 0.4188346862792969\n"
     ]
    }
   ],
   "source": [
    "# Evaluate phi\n",
    "\n",
    "## YOUR CODE HERE ##\n",
    "Phi = phi()\n",
    "print(f\"The evaluation of phi is: {Phi.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ffc37",
   "metadata": {},
   "source": [
    "### Part 4c. (0.5 pts) Discuss the results\n",
    "\n",
    "Was the satisfaction value what you expected to see? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5608b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "The satisfaction is way lower than expected, since we now that now formula is hyponym of themself, we know that the formula should hold. However in this model it seems to have a satisfaction o arounf 0.4 which suggest it does not hold. The error migth ensue because the model is still bad at handeling the negative examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba5b2a",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "In this worksheet, we looked at the hyponymy relation that can hold between words.\n",
    "\n",
    "1. We used Logical Neural Networks with a very small hyponym dataset to infer a set of facts, and discussed the kinds of facts that you can infer and the limitations of the model as it is implemented.\n",
    "2. We set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
    "3. We evaluated the effect of different axioms in the LTN system.\n",
    "4. And finally, you queried your model with new logical statements.\n",
    "\n",
    "For another 15 points, you can extend this worksheet in a number of different ways. \n",
    "\n",
    "### Possible extensions\n",
    "\n",
    "1. Use a new dataset for the task of inferring relationships over data.\n",
    "2. Use the same dataset with a different model that we have covered in class. You could potentially use Logical Neural Networks, although they are a little slow.\n",
    "3. Extend the investigation already started in this notebook. How do you expect the hyponymy relation to behave? Can you improve performance on novel queries?\n",
    "4. Extend this investigation by including semantic information into the word embeddings from external sources.\n",
    "5. Other ideas? Feel free to discuss with me!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2fbb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a05d463",
   "metadata": {},
   "source": [
    "The previous part of the note book highligthed the issue that the LTN does not generelisize well to novel formula's. The next part of the notebook will extend to on this by looking if the LTN is better capable of generelising to a novel formula, if it is trained on all subparts of the formula. And to what extend training on those subparts impact the formula. To show this we will train the model to understand a version of the transitivity formula; (Hyp(x,y) & Hyp(y,z)) -> NOT(hyp(z,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1e70e",
   "metadata": {},
   "source": [
    "This research is set up as follows, 4 models will be trained on different formulas, one on only the hyp(x,y) relation, one on hyp(x,y) and not(hyp(x,y)), one will also learn the transitivi (Hyp(x,y) & Hyp(y,z))-> Hyp(x,z) next to to the previuosly mentioned formulas. And the last model will just learn the (Hyp(x,y) & Hyp(y,z)) -> NOT(hyp(z,x)) relation to function as baseline. All models will be trained 5 times to account for the variance in training.  IF possible the model will be trained on 3 different amount of epochs (100, 300, 500) to account for an increase of training times with more formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "hyp_only = []\n",
    "Hyp_neg_only = []\n",
    "hyp_neg_transitivity = []\n",
    "baseline_psi = []\n",
    "\n",
    "def psi():\n",
    "    # Create variables p, q, and r and initialize with the values from 'word_embeddings'\n",
    "    ## YOUR CODE HERE ##\n",
    "    p = ltn.Variable('p', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    q = ltn.Variable('q', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    r = ltn.Variable('r', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    # Return the truth value of psi\n",
    "    ## YOUR CODE HERE ##\n",
    "    return Forall([p, q, r], \n",
    "                  Implies(And(Hyp(p, q), Hyp(q, r)), Not(Hyp(r, p))))\n",
    "# Set up a training loop for 300 epochs\n",
    "## YOUR CODE HERE ##\n",
    "# reinitialise the model\n",
    "Hyp = ltn.Predicate(ModelHyp())\n",
    "# training with only the positive hyponymy instances\n",
    "for i in range(5):\n",
    "    for epoch in range(300):    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "        sat_agg = SatAgg(\n",
    "            \n",
    "            #Positive instances of hyponymy\n",
    "            ## YOUR CODE HERE ##\n",
    "            *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "\n",
    "        )\n",
    "        # Calculate the loss and propagate backwards\n",
    "        ## YOUR CODE HERE ##\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print metrics every 100 epochs of training\n",
    "        ## YOUR CODE HERE ##\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")\n",
    "    psi_value = psi()\n",
    "    hyp_only.append(psi_value.item())\n",
    "\n",
    "# reinitialise the model\n",
    "Hyp = ltn.Predicate(ModelHyp())\n",
    "# training with both positive and negative hyponymy instances\n",
    "for i in range(5):\n",
    "    for epoch in range(300):    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "        sat_agg = SatAgg(\n",
    "            \n",
    "            #Positive instances of hyponymy\n",
    "            *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "            #Negative instances of hyponymy\n",
    "            *[Not(Hyp(word_embeddings[x], word_embeddings[y])) for x, y in neg_pairs],\n",
    "        )\n",
    "        # Calculate the loss and propagate backwards\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print metrics every 100 epochs of training\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")\n",
    "    psi_value = psi()\n",
    "    Hyp_neg_only.append(psi_value.item())\n",
    "# reinitialise the model\n",
    "Hyp = ltn.Predicate(ModelHyp())\n",
    "# training with both positive and negative hyponymy instances and transitivity axiom\n",
    "for i in range(5):\n",
    "    for epoch in range(300):    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "        x_ = ltn.Variable('x_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "        y_ = ltn.Variable('y_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "        z_ = ltn.Variable('z_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "        sat_agg = SatAgg(\n",
    "            \n",
    "            #Positive instances of hyponymy\n",
    "            *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "            #Negative instances of hyponymy\n",
    "            *[Not(Hyp(word_embeddings[x], word_embeddings[y])) for x, y in neg_pairs],\n",
    "            # Transitivity axiom\n",
    "            Forall([x_, y_, z_],\n",
    "                    Implies(And(Hyp(x_, y_), Hyp(y_, z_)), Hyp(x_, z_))\n",
    "        )\n",
    "        )\n",
    "        # Calculate the loss and propagate backwards\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print metrics every 100 epochs of training\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")\n",
    "    psi_value = psi()\n",
    "    hyp_neg_transitivity.append(psi_value.item())\n",
    "# reinitialise the model\n",
    "Hyp = ltn.Predicate(ModelHyp())\n",
    "# training with baseline: psi formula only\n",
    "for i in range(5):\n",
    "    for epoch in range(300):    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "        p = ltn.Variable('p', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "        q = ltn.Variable('q', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "        r = ltn.Variable('r', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "        sat_agg = SatAgg(\n",
    "            Forall([p, q, r], \n",
    "                  Implies(And(Hyp(p, q), Hyp(q, r)), Not(Hyp(r, p))))\n",
    "        )\n",
    "        # Calculate the loss and propagate backwards\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print metrics every 100 epochs of training\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")\n",
    "    psi_value = psi()\n",
    "    baseline_psi.append(psi_value.item())\n",
    "# Print the results\n",
    "print(\"Hyp only:\", hyp_only)\n",
    "print(\"Hyp and Neg only:\", Hyp_neg_only)\n",
    "print(\"Hyp, Neg and Transitivity:\", hyp_neg_transitivity)\n",
    "print(\"Baseline psi only:\", baseline_psi)\n",
    "\n",
    "# plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hyp_only, label='Hyp only')\n",
    "plt.plot(Hyp_neg_only, label='Hyp and Neg only')\n",
    "plt.plot(hyp_neg_transitivity, label\n",
    "='Hyp, Neg and Transitivity')   \n",
    "plt.plot(baseline_psi, label='Baseline psi only')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Psi value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
