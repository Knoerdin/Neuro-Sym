{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19197c84",
   "metadata": {},
   "source": [
    "# Practical Worksheet\n",
    "\n",
    "In this worksheet, we will be working with a small dataset of hyponym-hypernym pairs. Hyponymy is the `is-a` relation. So we will have pairs like `(cat, mammal)` meaning 'A cat is a mammal'. The hyponym is the more specific term (e.g., cat) and the hypernym is the more general term (e.g., mammal). In this notebook you will:\n",
    "\n",
    "1. (3 pts) Use Logical Neural Networks with a very small hyponym dataset to infer a set of facts. You will discuss the kinds of facts that you can infer and the limitations of the model as it is implemented\n",
    "2. (5 pts) Set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
    "3. (5 pts) Evaluate the effect of different axioms in the LTN system.\n",
    "4. (2 pts) Query your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15d0af",
   "metadata": {},
   "source": [
    "## Part 0. Setup\n",
    "Create an environment and install python 3.12, numpy, pandas, and scikit-learn.\n",
    "\n",
    "Install LNNs using `pip install git+https://github.com/IBM/LNN`\n",
    "\n",
    "Install LTNs using `pip install LTNtorch`\n",
    "\n",
    "Import packages as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03699930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ltntorch in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ltntorch) (1.26.4)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ltntorch) (2.9.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch->ltntorch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch->ltntorch) (4.12.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch->ltntorch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch->ltntorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch->ltntorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch->ltntorch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from sympy>=1.13.3->torch->ltntorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jinja2->torch->ltntorch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "!pip install ltntorch\n",
    "import ltn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84644f44",
   "metadata": {},
   "source": [
    "## Part 1. Inferring facts using Logical Neural Networks\n",
    "\n",
    "In this first part, we will manually specify a very small dictionary of hyponym facts. We have three hyponyms and three non-hyponyms. The hyponymy relation is transitive, meaning that if $x$ is a hyponym of $y$ and $y$ is a hyponym of $z$, then $x$ should be a hyponym of $z$.\n",
    "\n",
    "You will:\n",
    "\n",
    "a. (1.5 pt) Set up a LNN model with suitable variables, a transitivity axiom, and hyponymy data.\n",
    "\n",
    "b. (0.5 pt) Run inference over the model.\n",
    "\n",
    "c. (1 pt) Inspect the output of the model and discuss whether the output is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "032bfc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/IBM/LNN\n",
      "  Cloning https://github.com/IBM/LNN to /private/var/folders/n1/vr1f_0y52hj2hxn2mg_d76f80000gn/T/pip-req-build-980xro9f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/IBM/LNN /private/var/folders/n1/vr1f_0y52hj2hxn2mg_d76f80000gn/T/pip-req-build-980xro9f\n",
      "  Resolved https://github.com/IBM/LNN to commit 18ea03a52a79e6bbe8dada76e1ad9b320cd894d4\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jupyter in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (3.9.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.23.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.3.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (2.2.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (69.5.1)\n",
      "Requirement already satisfied: tabulate in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (0.9.0)\n",
      "Requirement already satisfied: torch>=2.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (2.9.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from lnn==1.0) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from matplotlib>=3.3.3->lnn==1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from pandas>=1.3.4->lnn==1.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from pandas>=1.3.4->lnn==1.0) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch>=2.7.1->lnn==1.0) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch>=2.7.1->lnn==1.0) (4.12.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch>=2.7.1->lnn==1.0) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch>=2.7.1->lnn==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from torch>=2.7.1->lnn==1.0) (2025.10.0)\n",
      "Requirement already satisfied: notebook in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter->lnn==1.0) (7.5.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter->lnn==1.0) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter->lnn==1.0) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter->lnn==1.0) (6.29.4)\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter->lnn==1.0) (8.1.8)\n",
      "Requirement already satisfied: jupyterlab in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter->lnn==1.0) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.3->lnn==1.0) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.7.1->lnn==1.0) (1.3.0)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (1.8.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (8.25.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (1.6.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (5.9.8)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (26.0.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (6.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipykernel->jupyter->lnn==1.0) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipywidgets->jupyter->lnn==1.0) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipywidgets->jupyter->lnn==1.0) (3.0.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jinja2->torch>=2.7.1->lnn==1.0) (2.1.5)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-console->jupyter->lnn==1.0) (3.0.46)\n",
      "Requirement already satisfied: pygments in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-console->jupyter->lnn==1.0) (2.18.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (0.28.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (0.2.4)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab->jupyter->lnn==1.0) (2.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbconvert->jupyter->lnn==1.0) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->lnn==1.0) (1.4.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter->lnn==1.0) (0.16.0)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->lnn==1.0) (4.2.2)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.5.3)\n",
      "Requirement already satisfied: overrides>=5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.23.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.15.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (4.22.0)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert->jupyter->lnn==1.0) (2.19.1)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter->lnn==1.0) (0.2.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->jupyter->lnn==1.0) (2.5)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (25.1.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (0.18.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (0.1.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter->lnn==1.0) (2.2.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->lnn==1.0) (0.2.2)\n",
      "Requirement already satisfied: fqdn in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (25.10.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (2.23)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/homebrew/Caskroom/miniconda/base/envs/jupyterbook/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->lnn==1.0) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "# We first set up a small dictionary of hyponyms\n",
    "!pip install git+https://github.com/IBM/LNN\n",
    "from lnn import Fact\n",
    "\n",
    "hyp_dict = {('cat', 'mammal'):Fact.TRUE,\n",
    "            ('dog', 'mammal'):Fact.TRUE,\n",
    "            ('mammal', 'animal'):Fact.TRUE,\n",
    "            ('cat', 'dog'):Fact.FALSE,\n",
    "            ('animal', 'mammal'):Fact.FALSE,\n",
    "            ('mammal', 'dog'):Fact.FALSE,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafd55d",
   "metadata": {},
   "source": [
    "### Part 1a) (1.5 pts) Setting up the model.\n",
    "Set up a LNN model with suitable predicates and variables, a transitivity axiom, and hyponymy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a13684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty model\n",
    "from lnn import Model\n",
    "model = Model()\n",
    "from lnn import Propositions, And, Implies, Iff, Fact, Model, Or\n",
    "A, B, C, D, E = Propositions(\"A\", \"B\", \"C\", \"D\", \"E\")\n",
    "IMPLIES=Implies(A, B)\n",
    "AND=And(C, D)\n",
    "IFF=Iff(AND, E)\n",
    "SENTENCE =And(IMPLIES, IFF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1697655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predicate of arity 2 called Hyps and three variables x, y, z\n",
    "## YOUR CODE HERE ##\n",
    "from lnn import Predicate, Variable\n",
    "Hyps = Predicate('Hyps', arity=2)\n",
    "x = Variable('x')\n",
    "y = Variable('y')\n",
    "z = Variable('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d25b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logical rule that encodes the fact that the hyponymy relation is transitive\n",
    "## YOUR CODE HERE ##\n",
    "transitivity_rule = Implies(And(Hyps(x, y), Hyps(y, z)), Hyps(x, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc34a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model()\n"
     ]
    }
   ],
   "source": [
    "# Add the knowledge and the data (the hyponymy dict) to the model and print.\n",
    "## YOUR CODE HERE ##\n",
    "model.add_knowledge(transitivity_rule)\n",
    "model.add_data({Hyps: {\n",
    "    ('cat', 'mammal'): Fact.TRUE,\n",
    "    ('dog', 'mammal'): Fact.TRUE,\n",
    "    ('mammal', 'animal'): Fact.TRUE,\n",
    "    ('cat', 'dog'): Fact.FALSE,\n",
    "    ('animal', 'mammal'): Fact.FALSE,\n",
    "    ('mammal', 'dog'): Fact.FALSE,\n",
    "}})\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42cce3",
   "metadata": {},
   "source": [
    "### Part 1b) (0.5 pts) Inferring facts\n",
    "Run inference over the model and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bf7202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***************************************************************************\n",
      "                                LNN Model\n",
      "\n",
      "OPEN Implies: ((Hyps(0, 1) ∧ Hyps(1, 2)) → Hyps(0, 2)) \n",
      "('cat', 'mammal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'cat', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('dog', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'animal')                              TRUE (1.0, 1.0)\n",
      "('animal', 'mammal', 'animal')                              TRUE (1.0, 1.0)\n",
      "('animal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('cat', 'dog', 'dog')                                       TRUE (1.0, 1.0)\n",
      "('cat', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'dog')                                    UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'dog')                                       TRUE (1.0, 1.0)\n",
      "('cat', 'animal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('dog', 'dog', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('dog', 'cat', 'dog')                                       TRUE (1.0, 1.0)\n",
      "('cat', 'dog', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'mammal', 'dog')                                 TRUE (1.0, 1.0)\n",
      "('cat', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'cat', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('dog', 'dog', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'cat')                                       TRUE (1.0, 1.0)\n",
      "('cat', 'cat', 'mammal')                                    TRUE (1.0, 1.0)\n",
      "('dog', 'mammal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'mammal')                           UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'mammal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'dog')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'mammal')                              TRUE (1.0, 1.0)\n",
      "('cat', 'mammal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'mammal')                              TRUE (1.0, 1.0)\n",
      "('mammal', 'dog', 'cat')                                    TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'cat')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'cat', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('dog', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('animal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'animal')                                    TRUE (1.0, 1.0)\n",
      "('animal', 'dog', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'mammal')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'animal', 'mammal')                              TRUE (1.0, 1.0)\n",
      "('cat', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'mammal', 'dog')                                    TRUE (1.0, 1.0)\n",
      "('cat', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'animal')                              TRUE (1.0, 1.0)\n",
      "\n",
      "OPEN And: (Hyps(0, 1) ∧ Hyps(1, 2)) \n",
      "('cat', 'mammal', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('dog', 'animal', 'mammal')                                FALSE (0.0, 0.0)\n",
      "('mammal', 'animal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'animal')                             FALSE (0.0, 0.0)\n",
      "('animal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'animal')                                FALSE (0.0, 0.0)\n",
      "('cat', 'dog', 'dog')                                      FALSE (0.0, 0.0)\n",
      "('cat', 'cat', 'dog')                                      FALSE (0.0, 0.0)\n",
      "('cat', 'animal', 'mammal')                                FALSE (0.0, 0.0)\n",
      "('dog', 'dog', 'mammal')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'dog')                                      FALSE (0.0, 0.0)\n",
      "('cat', 'dog', 'mammal')                                   FALSE (0.0, 0.0)\n",
      "('mammal', 'mammal', 'dog')                                FALSE (0.0, 0.0)\n",
      "('dog', 'cat', 'mammal')                                 UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'mammal')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('mammal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('animal', 'mammal', 'dog')                                FALSE (0.0, 0.0)\n",
      "('mammal', 'dog', 'mammal')                                FALSE (0.0, 0.0)\n",
      "('mammal', 'animal', 'mammal')                             FALSE (0.0, 0.0)\n",
      "('cat', 'mammal', 'animal')                                 TRUE (1.0, 1.0)\n",
      "('animal', 'mammal', 'mammal')                             FALSE (0.0, 0.0)\n",
      "('mammal', 'cat', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('dog', 'mammal', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('mammal', 'cat', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'animal')                                   FALSE (0.0, 0.0)\n",
      "('animal', 'dog', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'mammal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'mammal')                             FALSE (0.0, 0.0)\n",
      "('cat', 'mammal', 'dog')                                   FALSE (0.0, 0.0)\n",
      "('mammal', 'mammal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('cat', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'mammal', 'cat')                                FALSE (0.0, 0.0)\n",
      "('mammal', 'mammal', 'mammal')                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'mammal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'animal')                           UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal', 'dog')                              UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'animal', 'cat')                              UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'animal')                              UNKNOWN (0.0, 1.0)\n",
      "('cat', 'dog', 'cat')                                      FALSE (0.0, 0.0)\n",
      "('animal', 'dog', 'cat')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'animal')                                 UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'cat')                                    UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog', 'dog')                                    UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog', 'dog')                                 UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'dog', 'cat')                                   FALSE (0.0, 0.0)\n",
      "\n",
      "OPEN Predicate: Hyps \n",
      "('cat', 'mammal')                                           TRUE (1.0, 1.0)\n",
      "('animal', 'mammal')                                       FALSE (0.0, 0.0)\n",
      "('mammal', 'dog')                                          FALSE (0.0, 0.0)\n",
      "('dog', 'mammal')                                           TRUE (1.0, 1.0)\n",
      "('cat', 'dog')                                             FALSE (0.0, 0.0)\n",
      "('mammal', 'animal')                                        TRUE (1.0, 1.0)\n",
      "('cat', 'cat')                                           UNKNOWN (0.0, 1.0)\n",
      "('cat', 'animal')                                        UNKNOWN (0.0, 1.0)\n",
      "('animal', 'cat')                                        UNKNOWN (0.0, 1.0)\n",
      "('animal', 'animal')                                     UNKNOWN (0.0, 1.0)\n",
      "('animal', 'dog')                                        UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'cat')                                        UNKNOWN (0.0, 1.0)\n",
      "('mammal', 'mammal')                                     UNKNOWN (0.0, 1.0)\n",
      "('dog', 'cat')                                           UNKNOWN (0.0, 1.0)\n",
      "('dog', 'animal')                                        UNKNOWN (0.0, 1.0)\n",
      "('dog', 'dog')                                           UNKNOWN (0.0, 1.0)\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Part 1b (0.5 pts) Run inference over the model and print the output \n",
    "## YOUR CODE HERE ##\n",
    "model.infer()\n",
    "model.print()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0f233",
   "metadata": {},
   "source": [
    "### Part 1c) (1 pt) Inspecting the output.\n",
    "\n",
    "You should see that there are various facts whose truth value is unknown. \n",
    "\n",
    "\n",
    "Q1: Why can we not infer the truth value of all facts with the given database and axioms?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2: Suggest a suitable axiom to add to this system that would help to infer more facts. You do not need to implement the axiom.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84851554",
   "metadata": {},
   "source": [
    "\n",
    "Q1: Sometimes we do not knwo anything about all facts involved such as (mammel, mammel) and (mammel, cat) then we can not know (mammel, cat) since we have none of the fax and these are never mentioned so we cannot learn them either\n",
    "\n",
    "\n",
    "Q2: Implies(Hyps(x,y), not(Hyps(y,x))) if one is in one group sach cat in mammel then mammel not in cat. This should help create more false examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b131b6",
   "metadata": {},
   "source": [
    "## Part 2 (5 pts) Building Embeddings with Logic Tensor Networks.\n",
    "In this part, we will build a Logic Tensor Network to learn embeddings for the hyponyms. You will:\n",
    "\n",
    "a. (1 pt) Describe why learning embeddings for the hyponyms is a suitable approach.\n",
    "\n",
    "b. (1 pt) Set up a predicate for the hyponymy relation.\n",
    "\n",
    "c. (1 pt) Train a simple network on the hyponymy task.\n",
    "\n",
    "d. (2 pts) Assess satisfaction on the test set  and negative sample set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd42761",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "\n",
    "Below, we import the data into pandas dataframes. Take a look at the data to familiarise yourself with the format. In each .csv file we have a list of word pairs. \n",
    "- In train_hypernyms we have the set of hypernym pairs we will train on. \n",
    "- In test_hypernyms we have the set of pairs we will test on. \n",
    "- In non_hypernyms we have a set of word pairs that are not hypernym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36661dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs:\n",
      "[['supermarket' 'commercial building']\n",
      " ['hand tool' 'tool']\n",
      " ['peach' 'fruit']\n",
      " ['pike' 'fish']\n",
      " ['nail gun' 'power tool']]\n",
      "Testing pairs:\n",
      "[['workshop' 'building']\n",
      " ['train' 'vehicle']\n",
      " ['pine' 'physical object']\n",
      " ['snare drum' 'physical object']\n",
      " ['grape' 'physical object']]\n",
      "Negative pairs:\n",
      "[['jigsaw' 'nail gun']\n",
      " ['temple' 'synagogue']\n",
      " ['double bass' 'banjo']\n",
      " ['turkey' 'turkey']\n",
      " ['crocodile' 'snake']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('train_hypernyms.csv')\n",
    "test_df = pd.read_csv('test_hypernyms.csv')\n",
    "neg_df = pd.read_csv('non_hypernyms.csv')\n",
    "\n",
    "\n",
    "train_pairs = train_df.values\n",
    "test_pairs = test_df.values\n",
    "neg_pairs = neg_df.values\n",
    "\n",
    "print(\"Training pairs:\")\n",
    "print(train_pairs[:5])\n",
    "\n",
    "print(\"Testing pairs:\")\n",
    "print(test_pairs[:5])\n",
    "\n",
    "print(\"Negative pairs:\")\n",
    "print(neg_pairs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ad7aa",
   "metadata": {},
   "source": [
    "### Part 2a. (1 pt) Learning Embeddings\n",
    "\n",
    "When we use a logic tensor network, we can choose to use data from outside sources or to train embeddings within the network. We will be training embeddings. Do you think this is a suitable approach for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d647aef",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e33e8",
   "metadata": {},
   "source": [
    "Below, we will set up the vocabulary and the initial random word embeddings to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da40ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a set of vocab by taking the union of the hyponyms and hypernyms\n",
    "vocab = set(train_df.hyper.unique()).union(train_df.hypo.unique())\n",
    "\n",
    "# Set the dimension of the vocab to 10\n",
    "vocab_dim = 10\n",
    "\n",
    "# Build a dictionary of word embeddings initialised randomly and set to be trainable.\n",
    "word_embeddings = {word: ltn.Constant(torch.rand((vocab_dim,)), trainable=True) \\\n",
    "                   for word in vocab}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f706ae5",
   "metadata": {},
   "source": [
    "### Part 2b. (1 pt) Defining a predicate.\n",
    "Define a predicate as a feed-forward NN with ELU and sigmoid activation functions and one hidden layer of size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c8835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feed-forward NN  with ELU and sigmoid activation functions and one hidden layer of size 16.\n",
    "class ModelHyp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        ## YOUR CODE HERE ##    \n",
    "        super(ModelHyp, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        self.fc1 = torch.nn.Linear(2 * vocab_dim, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # Specify the forward pass with ELU on the hidden layers and sigmoid on the output\n",
    "        x = list(x)\n",
    "        x = torch.cat(x, dim=1)\n",
    "        ## YOUR CODE HERE ##\n",
    "        x = self.fc1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# Wrap the feed-forward NN to make it an LTN predicate called Hyp\n",
    "Hyp = ltn.Predicate(ModelHyp())\n",
    "x = ltn.Variable('x', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "y = ltn.Variable('y', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "\n",
    "# Define connectives, quantifiers, and SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e276e36",
   "metadata": {},
   "source": [
    "### Part 2c. (1 pt) Training the network\n",
    "\n",
    "We set up a simple network in which we view our knowledge base as consisting just of those pairs in the training set. So our knowledge base states that for each word pair in the training set, this is a hyponym pair. We want to maximise the satisfaction over this knowledge base. To do this, we write a suitable axiom to aggregate the satisfaction of the hyponymy predicate over these pairs, and train the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22e5c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.4785027503967285 | Train Sat 0.5214972496032715\n",
      " epoch 20 | loss 0.35377025604248047 | Train Sat 0.6462297439575195\n",
      " epoch 40 | loss 0.2161586880683899 | Train Sat 0.7838413119316101\n",
      " epoch 60 | loss 0.09590983390808105 | Train Sat 0.904090166091919\n",
      " epoch 80 | loss 0.03075575828552246 | Train Sat 0.9692442417144775\n",
      " epoch 100 | loss 0.007810056209564209 | Train Sat 0.9921899437904358\n",
      " epoch 120 | loss 0.0017260313034057617 | Train Sat 0.9982739686965942\n",
      " epoch 140 | loss 0.0003895759582519531 | Train Sat 0.999610424041748\n",
      " epoch 160 | loss 0.0001418590545654297 | Train Sat 0.9998581409454346\n",
      " epoch 180 | loss 0.00010526180267333984 | Train Sat 0.9998947381973267\n",
      " epoch 200 | loss 0.00010061264038085938 | Train Sat 0.9998993873596191\n",
      " epoch 220 | loss 0.00010007619857788086 | Train Sat 0.9998999238014221\n",
      " epoch 240 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 260 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 280 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "# We have to optimize the parameters of the predicate and also of the embeddings\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "for epoch in range(300):    \n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    sat_agg = SatAgg(\n",
    "# Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs]\n",
    "        ,\n",
    "        \n",
    "\n",
    "        # Our list of hyponym pairs is in train_pairs.\n",
    "        # We want to maximise the satisfaction gained by inputting the embeddings of those words into\n",
    "        # our hyponymy predicate\n",
    "        \n",
    "        \n",
    "\n",
    "    )\n",
    "    \n",
    "    loss = 1. - sat_agg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41705558",
   "metadata": {},
   "source": [
    "### Part 2d (2 pts) Assessing the satisfaction on the test set\n",
    "\n",
    "Calculate the satisfaction over the test set using SatAgg. Do you think the model is generalising well? Now calculate the satisfaction over the negative samples dataset. Is this a suitable satisfaction level? Why or why not?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "The model performs well on the test set, having a satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90053da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocab = set(test_df.hyper.unique()).union(test_df.hypo.unique())\n",
    "test_x = ltn.Variable('x', torch.stack([word_embeddings[word].value for word in test_vocab]))\n",
    "test_y = ltn.Variable('y', torch.stack([word_embeddings[word].value for word in test_vocab]))\n",
    "neg_vocab = set(neg_df.hyper.unique()).union(neg_df.hypo.unique())\n",
    "neg_x = ltn.Variable('x', torch.stack([word_embeddings[word].value for word in neg_vocab]))\n",
    "neg_y = ltn.Variable('y', torch.stack([word_embeddings[word].value for word in neg_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c63e4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the satisfaction of the test dataset is: 0.9998999834060669\n",
      "the satisfaction of the negative dataset is: 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "satisfaction_test = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in test_pairs],)\n",
    "satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs],)\n",
    "print(f\"the satisfaction of the test dataset is: {satisfaction_test}\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: {satisfaction_neg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b0390",
   "metadata": {},
   "source": [
    "## Part 3. (5 pts) Evaluate the effect of different axioms in the LTN system\n",
    "\n",
    "In this part you will:\n",
    "\n",
    "a. (2 pts) Retrain the model and evaluate the performance with negation included\n",
    "\n",
    "b. (2 pts) Retrain the model and evaluate performance with transitivity included\n",
    "\n",
    "c. (1 pt) Discuss the effect of the different axioms introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aaad1a",
   "metadata": {},
   "source": [
    "### Part 3a. (2pts)  Retraining the model with negation\n",
    "Reinitialise the model and retrain, including information from the `neg_pairs` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac747fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialise the model\n",
    "Hyp = ltn.Predicate(ModelHyp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "473942d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.4552726745605469 | Train Sat 0.5447273254394531\n",
      " epoch 20 | loss 0.3050692081451416 | Train Sat 0.6949307918548584\n",
      " epoch 40 | loss 0.1555114984512329 | Train Sat 0.8444885015487671\n",
      " epoch 60 | loss 0.05448251962661743 | Train Sat 0.9455174803733826\n",
      " epoch 80 | loss 0.013306796550750732 | Train Sat 0.9866932034492493\n",
      " epoch 100 | loss 0.002541482448577881 | Train Sat 0.9974585175514221\n",
      " epoch 120 | loss 0.0004667043685913086 | Train Sat 0.9995332956314087\n",
      " epoch 140 | loss 0.0001448988914489746 | Train Sat 0.999855101108551\n",
      " epoch 160 | loss 0.00010460615158081055 | Train Sat 0.9998953938484192\n",
      " epoch 180 | loss 0.0001004338264465332 | Train Sat 0.9998995661735535\n",
      " epoch 200 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 220 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 240 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 260 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n",
      " epoch 280 | loss 0.00010001659393310547 | Train Sat 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "for epoch in range(300):\n",
    "    \n",
    "    \n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "        ## YOUR CODE HERE ##\n",
    "        sat_agg = SatAgg(\n",
    "        # Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "        \n",
    "\n",
    "        # Implement one axiom which aggregates the satisfaction across the (x, y) in neg_pairs\n",
    "        # Note that this statement should involve a negation.\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs],)\n",
    "\n",
    "    # Calculate the loss and propagate backwards\n",
    "    ## YOUR CODE HERE ##\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    ## YOUR CODE HERE ##\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768815fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the satisfaction of the test dataset is: 0.9998999834060669\n",
      "the satisfaction of the negative dataset is: 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "# Calculate the satisfaction across the test dataset and the negated dataset\n",
    "satisfaction_test = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in test_pairs])\n",
    "satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs])\n",
    "print(f\"the satisfaction of the test dataset is: {satisfaction_test.item()}\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: {satisfaction_neg.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbc5e2",
   "metadata": {},
   "source": [
    "### Part 3b. (2 pts) Retraining the model with transitivity\n",
    "\n",
    "As we discussed in Part 1, the hyponymy relation is transitive. This should be reflected in the axioms. Reinitialise the model and add an axiom expressing the rule:\n",
    "\n",
    "$\\forall x, y, z Hyp(x, y) \\land Hyp(y, z) \\implies Hyp(x, z)$\n",
    "\n",
    "Retrain the model and evaluate on the test and negated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialise the model\n",
    "## YOUR CODE HERE ##\n",
    "Hyp = ltn.Predicate(ModelHyp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce13b58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 20 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 40 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 60 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 80 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 100 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 120 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 140 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 160 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 180 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 200 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 220 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 240 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 260 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n",
      " epoch 280 | loss 0.6057393550872803 | Train Sat 0.3942606449127197\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "## YOUR CODE HERE ##\n",
    "for epoch in range(300):    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "    x_ = ltn.Variable('x_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    y_ = ltn.Variable('y_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "    z_ = ltn.Variable('z_', torch.stack([word_embeddings[word].value for word in vocab]))\n",
    "\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "    sat_agg = SatAgg(\n",
    "        \n",
    "        #Positive instances of hyponymy\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in train_pairs],\n",
    "\n",
    "        #Negative instances of hyponymy\n",
    "        ## YOUR CODE HERE ##\n",
    "        *[Not(Hyp(word_embeddings[x], word_embeddings[y])) for x, y in neg_pairs],\n",
    "        \n",
    "        # Transitivity axiom\n",
    "        ## YOUR CODE HERE ##\n",
    "        Forall([x_, y_, z_],\n",
    "                Implies(And(Hyp(x_, y_), Hyp(y_, z_)), Hyp(x_, z_))\n",
    "\n",
    "    ))\n",
    "    # Calculate the loss and propagate backwards\n",
    "    ## YOUR CODE HERE ##\n",
    "    loss = 1. - sat_agg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    ## YOUR CODE HERE ##\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebcefe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the satisfaction of the test dataset is: 0.9998999834060669\n",
      "the satisfaction of the negative dataset is: 0.9998999834060669\n"
     ]
    }
   ],
   "source": [
    "# Calculate the satisfaction across the test dataset and the negated dataset\n",
    "satisfaction_test = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in test_pairs])\n",
    "satisfaction_neg = SatAgg(*[Hyp(word_embeddings[x], word_embeddings[y]) for x, y in neg_pairs])\n",
    "print(f\"the satisfaction of the test dataset is: {satisfaction_test.item()}\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: {satisfaction_neg.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d1248",
   "metadata": {},
   "source": [
    "### Part 3c. (1 pt)  Evaluating the model\n",
    "How has the satisfaction changed across the test set and the set of negative examples as you include different axioms? Why has this happened? Write a couple of sentences with your conclusions about the datasets and the model you have built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed9e73",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "The satisfacrtion is great from the start seeming that the dataset does not have a difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded3b40",
   "metadata": {},
   "source": [
    "## Part 4 (2 pts) Querying the model\n",
    "\n",
    "One of the strengths of Logic Tensor Networks is that you are able to query the models you have built. In this part you will:\n",
    "\n",
    "a. (0.5 pts) Define a logical statement that you expect to hold in your model.\n",
    "\n",
    "b. (1 pt) Query the model.\n",
    "\n",
    "c. (0.5 pts) Discuss your result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a26fbd",
   "metadata": {},
   "source": [
    "### Part 4a. (0.5 pts) Defining a query\n",
    "\n",
    "Thinking about the properties of hyponymy, give a logical statement that you would expect to hold in your model. The statement can be quite simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a874b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53450e",
   "metadata": {},
   "source": [
    "### Part 4b. (1 pt) Querying the model\n",
    "\n",
    "Write a function that returns the satisfaction level of your logical statement and determine the satisfaction level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daddef07",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (917430481.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    ## YOUR CODE HERE ##\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# this function returns the satisfaction level of your logical formula\n",
    "def phi():\n",
    "    # Create variables p, q, and r and initialize with the values from 'word_embeddings'\n",
    "    ## YOUR CODE HERE ##\n",
    "    # Return the truth value of phi\n",
    "    ## YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate phi\n",
    "\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ffc37",
   "metadata": {},
   "source": [
    "### Part 4c. (0.5 pts) Discuss the results\n",
    "\n",
    "Was the satisfaction value what you expected to see? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5608b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba5b2a",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "In this worksheet, we looked at the hyponymy relation that can hold between words.\n",
    "\n",
    "1. We used Logical Neural Networks with a very small hyponym dataset to infer a set of facts, and discussed the kinds of facts that you can infer and the limitations of the model as it is implemented.\n",
    "2. We set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
    "3. We evaluated the effect of different axioms in the LTN system.\n",
    "4. And finally, you queried your model with new logical statements.\n",
    "\n",
    "For another 15 points, you can extend this worksheet in a number of different ways. \n",
    "\n",
    "### Possible extensions\n",
    "\n",
    "1. Use a new dataset for the task of inferring relationships over data.\n",
    "2. Use the same dataset with a different model that we have covered in class. You could potentially use Logical Neural Networks, although they are a little slow.\n",
    "3. Extend the investigation already started in this notebook. How do you expect the hyponymy relation to behave? Can you improve performance on novel queries?\n",
    "4. Extend this investigation by including semantic information into the word embeddings from external sources.\n",
    "5. Other ideas? Feel free to discuss with me!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2fbb1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jupyterbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
