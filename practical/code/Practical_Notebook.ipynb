{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19197c84",
   "metadata": {},
   "source": [
    "# Practical Worksheet\n",
    "\n",
    "In this worksheet, we will be working with a small dataset of hyponym-hypernym pairs. Hyponymy is the `is-a` relation. So we will have pairs like `(cat, mammal)` meaning 'A cat is a mammal'. The hyponym is the more specific term (e.g., cat) and the hypernym is the more general term (e.g., mammal). In this notebook you will:\n",
    "\n",
    "1. (3 pts) Use Logical Neural Networks with a very small hyponym dataset to infer a set of facts. You will discuss the kinds of facts that you can infer and the limitations of the model as it is implemented\n",
    "2. (5 pts) Set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
    "3. (5 pts) Evaluate the effect of different axioms in the LTN system.\n",
    "4. (2 pts) Query your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15d0af",
   "metadata": {},
   "source": [
    "## Part 0. Setup\n",
    "Create an environment and install python 3.12, numpy, pandas, and scikit-learn.\n",
    "\n",
    "Install LNNs using `pip install git+https://github.com/IBM/LNN`\n",
    "\n",
    "Install LTNs using `pip install LTNtorch`\n",
    "\n",
    "Import packages as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03699930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import numpy as np\n",
    "import ltn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84644f44",
   "metadata": {},
   "source": [
    "## Part 1. Inferring facts using Logical Neural Networks\n",
    "\n",
    "In this first part, we will manually specify a very small dictionary of hyponym facts. We have three hyponyms and three non-hyponyms. The hyponymy relation is transitive, meaning that if $x$ is a hyponym of $y$ and $y$ is a hyponym of $z$, then $x$ should be a hyponym of $z$.\n",
    "\n",
    "You will:\n",
    "\n",
    "a. (1.5 pt) Set up a LNN model with suitable variables, a transitivity axiom, and hyponymy data.\n",
    "\n",
    "b. (0.5 pt) Run inference over the model.\n",
    "\n",
    "c. (1 pt) Inspect the output of the model and discuss whether the output is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "032bfc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first set up a small dictionary of hyponyms\n",
    "from lnn import Fact\n",
    "\n",
    "hyp_dict = {('cat', 'mammal'):Fact.TRUE,\n",
    "            ('dog', 'mammal'):Fact.TRUE,\n",
    "            ('mammal', 'animal'):Fact.TRUE,\n",
    "            ('cat', 'dog'):Fact.FALSE,\n",
    "            ('animal', 'mammal'):Fact.FALSE,\n",
    "            ('mammal', 'dog'):Fact.FALSE,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafd55d",
   "metadata": {},
   "source": [
    "### Part 1a) (1.5 pts) Setting up the model.\n",
    "Set up a LNN model with suitable predicates and variables, a transitivity axiom, and hyponymy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a13684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HyponymNetwork(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2*d, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x, y):\n",
    "        return self.net(torch.cat([x, y], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1697655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predicate of arity 2 called Hyps and three variables x, y, z\n",
    "## YOUR CODE HERE ##\n",
    "# All possible words\n",
    "domain = sorted({x for key in hyp_dict for x in key})\n",
    "# Give number to all words\n",
    "sym2idx = {s:i for i,s in enumerate(domain)}\n",
    "n = len(domain)\n",
    "\n",
    "individuals = torch.stack([\n",
    "    F.one_hot(torch.tensor(sym2idx[s]), num_classes=n).float()\n",
    "    for s in domain\n",
    "])\n",
    "\n",
    "X = ltn.Variable(\"X\", individuals)\n",
    "Y = ltn.Variable(\"Y\", individuals)\n",
    "Z = ltn.Variable(\"Z\", individuals)\n",
    "\n",
    "C = {s: ltn.Constant(individuals[sym2idx[s]]) for s in domain}\n",
    "\n",
    "# Add the predicate\n",
    "Hyps = ltn.Predicate(model=HyponymNetwork(d=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9d25b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logical rule that encodes the fact that the hyponymy relation is transitive\n",
    "## YOUR CODE HERE ##\n",
    "# Add the needed conectives\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "\n",
    "# Forall[x,y,z]((hyp(x,y) and hyp(y,z)) -> hyp(x,z))\n",
    "transitivity = Forall([X, Y, Z],\n",
    "    Implies(And(Hyps(X, Y), Hyps(Y, Z)), Hyps(X, Z))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfc34a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the knowledge and the data (the hyponymy dict) to the model and print.\n",
    "## YOUR CODE HERE ##\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "true_facts = []\n",
    "false_facts = []\n",
    "\n",
    "for (a, b), value in hyp_dict.items():\n",
    "    if value == Fact.TRUE:\n",
    "        true_facts.append(Hyps(C[a], C[b]))\n",
    "    else:\n",
    "        false_facts.append(Not(Hyps(C[a], C[b])))\n",
    "\n",
    "# Aggregating the truth values of the axiom\n",
    "data = SatAgg(transitivity, *true_facts, *false_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42cce3",
   "metadata": {},
   "source": [
    "### Part 1b) (0.5 pts) Inferring facts\n",
    "Run inference over the model and print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58bf7202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global satisfaction: 0.5420331954956055\n",
      "\n",
      "Hyps(animal, animal): 0.4774361252784729\n",
      "Hyps(animal, cat): 0.4223968982696533\n",
      "Hyps(animal, dog): 0.44209885597229004\n",
      "Hyps(animal, mammal): 0.45686227083206177\n",
      "Hyps(cat, animal): 0.4569424092769623\n",
      "Hyps(cat, cat): 0.4297742545604706\n",
      "Hyps(cat, dog): 0.43440279364585876\n",
      "Hyps(cat, mammal): 0.4539198577404022\n",
      "Hyps(dog, animal): 0.47335493564605713\n",
      "Hyps(dog, cat): 0.44004902243614197\n",
      "Hyps(dog, dog): 0.4624500870704651\n",
      "Hyps(dog, mammal): 0.4647778868675232\n",
      "Hyps(mammal, animal): 0.47959277033805847\n",
      "Hyps(mammal, cat): 0.4342584013938904\n",
      "Hyps(mammal, dog): 0.4497668147087097\n",
      "Hyps(mammal, mammal): 0.462757408618927\n"
     ]
    }
   ],
   "source": [
    "# Part 1b (0.5 pts) Run inference over the model and print the output \n",
    "## YOUR CODE HERE ##\n",
    "print(f'Global satisfaction: {data.item()}\\n')\n",
    "\n",
    "for atom1 in domain:\n",
    "    for atom2 in domain:\n",
    "        q = Hyps(C[atom1], C[atom2])\n",
    "        print(f'Hyps({atom1}, {atom2}): {q.value.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0f233",
   "metadata": {},
   "source": [
    "### Part 1c) (1 pt) Inspecting the output.\n",
    "\n",
    "You should see that there are various facts whose truth value is unknown. \n",
    "\n",
    "Q1: Why can we not infer the truth value of all facts with the given database and axioms?\n",
    "\n",
    "Q2: Suggest a suitable axiom to add to this system that would help to infer more facts. You do not need to implement the axiom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ff74d",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b131b6",
   "metadata": {},
   "source": [
    "## Part 2 (5 pts) Building Embeddings with Logic Tensor Networks.\n",
    "In this part, we will build a Logic Tensor Network to learn embeddings for the hyponyms. You will:\n",
    "\n",
    "a. (1 pt) Describe why learning embeddings for the hyponyms is a suitable approach.\n",
    "\n",
    "b. (1 pt) Set up a predicate for the hyponymy relation.\n",
    "\n",
    "c. (1 pt) Train a simple network on the hyponymy task.\n",
    "\n",
    "d. (2 pts) Assess satisfaction on the test set  and negative sample set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd42761",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "\n",
    "Below, we import the data into pandas dataframes. Take a look at the data to familiarise yourself with the format. In each .csv file we have a list of word pairs. \n",
    "- In train_hypernyms we have the set of hypernym pairs we will train on. \n",
    "- In test_hypernyms we have the set of pairs we will test on. \n",
    "- In non_hypernyms we have a set of word pairs that are not hypernym pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36661dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs:\n",
      "[['supermarket' 'commercial building']\n",
      " ['hand tool' 'tool']\n",
      " ['peach' 'fruit']\n",
      " ['pike' 'fish']\n",
      " ['nail gun' 'power tool']]\n",
      "Testing pairs:\n",
      "[['workshop' 'building']\n",
      " ['train' 'vehicle']\n",
      " ['pine' 'physical object']\n",
      " ['snare drum' 'physical object']\n",
      " ['grape' 'physical object']]\n",
      "Negative pairs:\n",
      "[['jigsaw' 'nail gun']\n",
      " ['temple' 'synagogue']\n",
      " ['double bass' 'banjo']\n",
      " ['turkey' 'turkey']\n",
      " ['crocodile' 'snake']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_dir = '../data/'\n",
    "\n",
    "train_df = pd.read_csv(f'{data_dir}train_hypernyms.csv')\n",
    "test_df = pd.read_csv(f'{data_dir}test_hypernyms.csv')\n",
    "neg_df = pd.read_csv(f'{data_dir}non_hypernyms.csv')\n",
    "\n",
    "\n",
    "train_pairs = train_df.values\n",
    "test_pairs = test_df.values\n",
    "neg_pairs = neg_df.values\n",
    "\n",
    "print(\"Training pairs:\")\n",
    "print(train_pairs[:5])\n",
    "\n",
    "print(\"Testing pairs:\")\n",
    "print(test_pairs[:5])\n",
    "\n",
    "print(\"Negative pairs:\")\n",
    "print(neg_pairs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ad7aa",
   "metadata": {},
   "source": [
    "### Part 2a. (1 pt) Learning Embeddings\n",
    "\n",
    "When we use a logic tensor network, we can choose to use data from outside sources or to train embeddings within the network. We will be training embeddings. Do you think this is a suitable approach for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d647aef",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e33e8",
   "metadata": {},
   "source": [
    "Below, we will set up the vocabulary and the initial random word embeddings to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da40ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a set of vocab by taking the union of the hyponyms and hypernyms\n",
    "vocab = set(train_df.hyper.unique()).union(train_df.hypo.unique())\n",
    "\n",
    "# Set the dimension of the vocab to 10\n",
    "vocab_dim = 10\n",
    "\n",
    "# Build a dictionary of word embeddings initialised randomly and set to be trainable.\n",
    "word_embeddings = {word: ltn.Constant(torch.rand((vocab_dim,)), trainable=True) \\\n",
    "                   for word in vocab}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f706ae5",
   "metadata": {},
   "source": [
    "### Part 2b. (1 pt) Defining a predicate.\n",
    "Define a predicate as a feed-forward NN with ELU and sigmoid activation functions and one hidden layer of size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71c8835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feed-forward NN  with ELU and sigmoid activation functions and one hidden layer of size 16.\n",
    "class ModelHyp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        ## YOUR CODE HERE ##\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2*10, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, *x):\n",
    "        # Specify the forward pass with ELU on the hidden layers and sigmoid on the output\n",
    "        x = list(x)\n",
    "        x = torch.cat(x, dim=1)\n",
    "        ## YOUR CODE HERE ##\n",
    "        return self.net(x, dim=-1)\n",
    "    \n",
    "# Wrap the feed-forward NN to make it an LTN predicate called Hyp\n",
    "Hyp = ltn.Predicate(model=ModelHyp())\n",
    "\n",
    "# Define connectives, quantifiers, and SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e276e36",
   "metadata": {},
   "source": [
    "### Part 2c. (1 pt) Training the network\n",
    "\n",
    "We set up a simple network in which we view our knowledge base as consisting just of those pairs in the training set. So our knowledge base states that for each word pair in the training set, this is a hyponym pair. We want to maximise the satisfaction over this knowledge base. To do this, we write a suitable axiom to aggregate the satisfaction of the hyponymy predicate over these pairs, and train the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to optimize the parameters of the predicate and also of the embeddings\n",
    "params = list(Hyp.parameters()) +[i.value for i in word_embeddings.values()]\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "for epoch in range(300):    \n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    sat_agg = SatAgg(\n",
    "# Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
    "        ## YOUR CODE HERE ##\n",
    "        # Our list of hyponym pairs is in train_pairs.\n",
    "        # We want to maximise the satisfaction gained by inputting the embeddings of those words into\n",
    "        # our hyponymy predicate.\n",
    "        \n",
    "\n",
    "    )\n",
    "    \n",
    "    loss = 1. - sat_agg\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\" epoch {epoch} | loss {loss} | Train Sat {sat_agg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41705558",
   "metadata": {},
   "source": [
    "### Part 2d (2 pts) Assessing the satisfaction on the test set\n",
    "\n",
    "Calculate the satisfaction over the test set using SatAgg. Do you think the model is generalising well? Now calculate the satisfaction over the negative samples dataset. Is this a suitable satisfaction level? Why or why not?\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c63e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the satisfaction of the test dataset is: ## YOUR CODE HERE ##\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: ## YOUR CODE HERE ##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b0390",
   "metadata": {},
   "source": [
    "## Part 3. (5 pts) Evaluate the effect of different axioms in the LTN system\n",
    "\n",
    "In this part you will:\n",
    "\n",
    "a. (2 pts) Retrain the model and evaluate the performance with negation included\n",
    "\n",
    "b. (2 pts) Retrain the model and evaluate performance with transitivity included\n",
    "\n",
    "c. (1 pt) Discuss the effect of the different axioms introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aaad1a",
   "metadata": {},
   "source": [
    "### Part 3a. (2pts)  Retraining the model with negation\n",
    "Reinitialise the model and retrain, including information from the `neg_pairs` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac747fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialise the model\n",
    "Hyp = ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473942d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "    ## YOUR CODE HERE ##\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "        ## YOUR CODE HERE ##\n",
    "        # Implement one axiom which aggregates the satisfaction across the (x, y) in train_pairs\n",
    "        ## YOUR CODE HERE ##\n",
    "\n",
    "        # Implement one axiom which aggregates the satisfaction across the (x, y) in neg_pairs\n",
    "        # Note that this statement should involve a negation.\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "    \n",
    "    # Calculate the loss and propagate backwards\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768815fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the satisfaction across the test dataset and the negated dataset\n",
    "print(f\"the satisfaction of the test dataset is: ## YOUR CODE HERE ##\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: ## YOUR CODE HERE ##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdbc5e2",
   "metadata": {},
   "source": [
    "### Part 3b. (2 pts) Retraining the model with transitivity\n",
    "\n",
    "As we discussed in Part 1, the hyponymy relation is transitive. This should be reflected in the axioms. Reinitialise the model and add an axiom expressing the rule:\n",
    "\n",
    "$\\forall x, y, z Hyp(x, y) \\land Hyp(y, z) \\implies Hyp(x, z)$\n",
    "\n",
    "Retrain the model and evaluate on the test and negated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialise the model\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce13b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameters and optimizer\n",
    "## YOUR CODE HERE ##\n",
    "\n",
    "# Set up a training loop for 300 epochs\n",
    "## YOUR CODE HERE ##\n",
    "    \n",
    "    # Create variables x_, y_, and z_, grounded with values from the `word_embeddings` dictionary\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "    # Set up a variable sat_agg which is the result of aggregating the truth values of all the axioms\n",
    "    ## YOUR CODE HERE ##\n",
    "        \n",
    "        #Positive instances of hyponymy\n",
    "        ## YOUR CODE HERE ##\n",
    "\n",
    "        #Negative instances of hyponymy\n",
    "        ## YOUR CODE HERE ##\n",
    "        \n",
    "        # Transitivity axiom\n",
    "        ## YOUR CODE HERE ##\n",
    "\n",
    "    \n",
    "    # Calculate the loss and propagate backwards\n",
    "    ## YOUR CODE HERE ##\n",
    "\n",
    "    # Print metrics every 20 epochs of training\n",
    "    ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcefe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the satisfaction across the test dataset and the negated dataset\n",
    "print(f\"the satisfaction of the test dataset is: ## YOUR CODE HERE ##\")\n",
    "\n",
    "print(f\"the satisfaction of the negative dataset is: ## YOUR CODE HERE ##\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d1248",
   "metadata": {},
   "source": [
    "### Part 3c. (1 pt)  Evaluating the model\n",
    "How has the satisfaction changed across the test set and the set of negative examples as you include different axioms? Why has this happened? Write a couple of sentences with your conclusions about the datasets and the model you have built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ed9e73",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded3b40",
   "metadata": {},
   "source": [
    "## Part 4 (2 pts) Querying the model\n",
    "\n",
    "One of the strengths of Logic Tensor Networks is that you are able to query the models you have built. In this part you will:\n",
    "\n",
    "a. (0.5 pts) Define a logical statement that you expect to hold in your model.\n",
    "\n",
    "b. (1 pt) Query the model.\n",
    "\n",
    "c. (0.5 pts) Discuss your result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a26fbd",
   "metadata": {},
   "source": [
    "### Part 4a. (0.5 pts) Defining a query\n",
    "\n",
    "Thinking about the properties of hyponymy, give a logical statement that you would expect to hold in your model. The statement can be quite simple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a874b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53450e",
   "metadata": {},
   "source": [
    "### Part 4b. (1 pt) Querying the model\n",
    "\n",
    "Write a function that returns the satisfaction level of your logical statement and determine the satisfaction level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns the satisfaction level of your logical formula\n",
    "def phi():\n",
    "    # Create variables p, q, and r and initialize with the values from 'word_embeddings'\n",
    "    ## YOUR CODE HERE ##\n",
    "    # Return the truth value of phi\n",
    "    ## YOUR CODE HERE ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate phi\n",
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ffc37",
   "metadata": {},
   "source": [
    "### Part 4c. (0.5 pts) Discuss the results\n",
    "\n",
    "Was the satisfaction value what you expected to see? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5608b",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba5b2a",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "In this worksheet, we looked at the hyponymy relation that can hold between words.\n",
    "\n",
    "1. We used Logical Neural Networks with a very small hyponym dataset to infer a set of facts, and discussed the kinds of facts that you can infer and the limitations of the model as it is implemented.\n",
    "2. We set up a Logic Tensor Network to learn word embeddings and predicates that can model a larger hyponymy dataset.\n",
    "3. We evaluated the effect of different axioms in the LTN system.\n",
    "4. And finally, you queried your model with new logical statements.\n",
    "\n",
    "For another 15 points, you can extend this worksheet in a number of different ways. \n",
    "\n",
    "### Possible extensions\n",
    "\n",
    "1. Use a new dataset for the task of inferring relationships over data.\n",
    "2. Use the same dataset with a different model that we have covered in class. You could potentially use Logical Neural Networks, although they are a little slow.\n",
    "3. Extend the investigation already started in this notebook. How do you expect the hyponymy relation to behave? Can you improve performance on novel queries?\n",
    "4. Extend this investigation by including semantic information into the word embeddings from external sources.\n",
    "5. Other ideas? Feel free to discuss with me!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2fbb1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nesy_practical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
